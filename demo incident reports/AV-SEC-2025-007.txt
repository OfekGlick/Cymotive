Incident ID: AV-SEC-2025-007 Date of Detection: 2025-11-01 23:10 UTC Vehicle ID: AV-688G (Fleet: "Metro Shuttle") Threat Category: Adversarial Machine Learning Attack on Perception System Detection Method: Model Behavior Monitoring and Input Sanitization.

Detailed Incident Description: Vehicle AV-688G, while on its late-night route, experienced a series of bizarre object classification errors. The vehicle, approaching a stop sign, suddenly classified it as a "Speed Limit 80 mph" sign. A few moments later, it misclassified a pedestrian in a crosswalk as a "traffic cone." These rapid, nonsensical re-classifications were flagged by our perception integrity monitor. The monitor detected that the confidence scores from the primary camera's object detection model (a YOLOv5 derivative) were fluctuating wildly despite no significant changes in the input video stream. Analysis of the camera feed revealed that an attacker had placed a series of specially designed adhesive patches—an adversarial patch—onto the real stop sign. These patches, while appearing as random noise to a human observer, were crafted to exploit vulnerabilities in the neural network, causing it to produce a specific, incorrect classification with high confidence. This is a physical adversarial attack.

Impact Assessment: The potential impact was extremely high. Had the vehicle acted on the misclassification of the stop sign, it would have proceeded through an intersection without stopping, likely causing a collision. The misclassification of a pedestrian is also a critical safety failure. The vehicle's redundancy systems prevented an accident.

Response and Forensic Analysis: The vehicle's "Decision Logic Arbitrator" module flagged the classification from the camera-based perception system as highly anomalous and contradictory to both the HD map data (which indicated a stop sign at that location) and the radar data (which detected a large, stationary object). The system prioritized the HD map and radar data, correctly identifying the need to stop. It raised a Level 4 alert to the SOC, and a human operator remotely confirmed the presence of the stop sign. The vehicle was placed into a limited-capability mode until the next maintenance cycle. Forensic analysis of the stored video frames containing the adversarial patch allowed our AI security team to reverse-engineer the attack and generate a signature for this specific pattern. It appears the patch was designed to target the specific architecture of the YOLOv5 model used in that generation of our vehicles.

Recommendations:

    Implement adversarial training techniques into the machine learning model development lifecycle, using generated patterns like the one observed to make the models more robust.

    Deploy input sanitization filters on the camera feed, such as spatial smoothing or JPEG compression, which can sometimes disrupt the effectiveness of adversarial patterns before they reach the model.

    Enhance the decision logic to further entrench the principle of "distrusting and verifying" classifications from a single sensor source when they conflict with strong priors from other sources like HD maps.